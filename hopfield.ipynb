{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hopfield Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation for asynchronous Hopfield neural network\n",
    "# Note: memory capacity â‰ƒ 0.14*nodes\n",
    "class Hopfield_Neural_Network:\n",
    "    def __init__(self,nodes,iterations=100,weights=None):\n",
    "        self.nodes = nodes\n",
    "        self.iterations = iterations\n",
    "        try:\n",
    "            if weights == None:\n",
    "                self.weights = np.zeros((nodes,nodes))\n",
    "        except ValueError:\n",
    "            self.weights = weights\n",
    "    \n",
    "    def store(self,input):\n",
    "        dW = np.outer(input.transpose(),input)\n",
    "        np.fill_diagonal(dW,0)\n",
    "        self.weights += dW\n",
    "        \n",
    "    def recall(self,input):\n",
    "        update_sequence = np.random.choice(self.nodes, self.iterations)\n",
    "        for node in update_sequence:\n",
    "            input[node] = np.sign(np.inner(input,self.weights[:,node]))\n",
    "        return input\n",
    "    \n",
    "    def setIter(self,iter_):\n",
    "        self.iterations = iter_\n",
    "    \n",
    "    def save_weights(self,filename):\n",
    "        np.save(filename, self.weights)\n",
    "        \n",
    "    def load_weights(self,filename):\n",
    "        weights = np.load(filename)\n",
    "        if weights.shape == (self.nodes, self.nodes):\n",
    "            self.weights = weights\n",
    "        else:\n",
    "            raise ValueError(\"Dimensions of specified weight array does not match network weight dimensions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hnn = Hopfield_Neural_Network(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class matrix_expansion:\n",
    "    \n",
    "    def __init__(self,active_motors):\n",
    "        self.active_motors = active_motors\n",
    "        active_sensors = np.array(active_motors*2)\n",
    "        active_sensors[len(active_motors):] += 14\n",
    "        self.active_sensors = active_sensors\n",
    "        self.shape = ()\n",
    "        \n",
    "    def load_from_file(self,filename):\n",
    "        f = open(filename,\"r\")\n",
    "        matrix = f.read()\n",
    "        f.close()\n",
    "        matrix = re.split(\",NEW_ROW,\",matrix)\n",
    "        matrix.pop()\n",
    "        matrix = np.array([np.array(re.split(\",\", row)).astype(np.float) for row in matrix])\n",
    "        self.shape = matrix.shape\n",
    "        return matrix\n",
    "        \n",
    "    def reduced_matrix(self,matrix):\n",
    "        matrix = matrix[:,self.active_sensors][self.active_motors]\n",
    "        return matrix\n",
    "    \n",
    "    def expanded_matrix(self,reduced_matrix):\n",
    "        matrix = np.zeros(self.shape)\n",
    "        flat = reduced_matrix.flatten()\n",
    "        matrix = np.zeros((14,28))\n",
    "        k = 0\n",
    "        for i in active_motors:\n",
    "            for j in active_sensors:\n",
    "                matrix[i,j] = flat[k]\n",
    "                k += 1\n",
    "        return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_motors = [1,3,4,5,10,12]\n",
    "expander = matrix_expansion(active_motors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Front back\n",
    "filename = \"/home/markus/dep/dep_matrices/front_back.dep\"\n",
    "fb_matrix = expander.load_from_file(filename)\n",
    "fb_reduced = expander.reduced_matrix(fb_matrix)\n",
    "#fb_expanded = expander.expanded_matrix(fb_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Front side\n",
    "filename = \"/home/markus/dep/dep_matrices/front_side.dep\"\n",
    "fs_matrix = expander.load_from_file(filename)\n",
    "fs_reduced = expander.reduced_matrix(fs_matrix)\n",
    "#fs_expanded = expander.expanded_matrix(fs_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side down\n",
    "filename = \"/home/markus/dep/dep_matrices/side_down.dep\"\n",
    "sd_matrix = expander.load_from_file(filename)\n",
    "sd_reduced = expander.reduced_matrix(sd_matrix)\n",
    "#sd_expanded = expander.expanded_matrix(sd_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero\n",
    "zero_reduced = np.zeros(fb_reduced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrices = {\"fb\": fb_reduced, \"fs\": fs_reduced, \"sd\": sd_reduced, \"zero\": zero_reduced}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transition points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors = [\"zero\",\"fb\",\"fs\",\"sd\"]\n",
    "transitions = {(\"fb\",\"fs\"): [], (\"fb\",\"sd\"): [], (\"fs\",\"fb\"): [], (\"fs\",\"sd\"): [], (\"sd\",\"fb\"): [], (\"sd\",\"fs\"): []}\n",
    "\n",
    "# generate all transition combinations (including zero and \"self-transitions\")\n",
    "#transitions = {}\n",
    "#for behavior_1 in behaviors:\n",
    "#    for behavior_2 in behaviors:\n",
    "#        transitions[(behavior_1,behavior_2)] = []\n",
    "\n",
    "#transitions_for_muscle_2 = -1.5 rad, positive velocity for all behaviors, except zero for which it is 0,0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to get positions and velocities for fb, fs and sd at -1.5 going up, and set them for zero at 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb = pickle.load(open(\"/home/markus/dep/dep_data/bases/fb.pickle\",\"rb\"))\n",
    "fs = pickle.load(open(\"/home/markus/dep/dep_data/bases/fs.pickle\",\"rb\"))\n",
    "sd = pickle.load(open(\"/home/markus/dep/dep_data/bases/sd.pickle\", \"rb\"))\n",
    "zero = []\n",
    "for array in fb:\n",
    "    zero.append(np.zeros(array.shape))\n",
    "\n",
    "bases = {\"fb\": fb, \"fs\": fs, \"sd\": sd}#, \"zero\": zero}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtained from plot, time indices that meat transition_muscle_2 condition\n",
    "fb_t = 124\n",
    "fs_t = 126\n",
    "sd_t = 117\n",
    "zero_t = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos data\n",
    "pos = {\"fb\": fb[0][fb_t][active_motors], \"fs\": fs[0][fs_t][active_motors], \"sd\": sd[0][sd_t][active_motors], \"zero\": zero[0][zero_t][active_motors]}\n",
    "# vel data\n",
    "vel = {\"fb\": fb[1][fb_t][active_motors], \"fs\": fs[1][fs_t][active_motors], \"sd\": sd[1][sd_t][active_motors], \"zero\": zero[1][zero_t][active_motors]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brain ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(behaviors)\n",
    "start = 0.0\n",
    "width = 1.0/float(n)\n",
    "brain_ranges = {}\n",
    "for i in range(1,n+1):\n",
    "    brain_ranges[behaviors[i-1]] = ((i-1)*width-start, i*width-start)\n",
    "brain_id = {}\n",
    "for behavior in behaviors:\n",
    "    brain_id[behavior] = (brain_ranges[behavior][0]+brain_ranges[behavior][1])/2\n",
    "# \"zero = [0.0,0.25], \"fb\" = [0.25,0.5], \"fs\" = [0.5, 0.75], \"sd\" = [0.75,1.00]\n",
    "#brain_id = {\"zero\": 0.125, \"fb\": 0.375, \"fs\": 0.625, \"sd\": 0.875}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTM SDR Scalar Encoder\n",
    "# Input: Scalar\n",
    "# Parameters: n - number of units, w - bits used to represent signal (width), b - buckets (i.e. resolution), \n",
    "#             min - minimum value of input (inclusive), max - maximum input value (inclusive)\n",
    "class scalar_sdr:\n",
    "    \n",
    "    def __init__(self, b, w, min_, max_, shape=0):\n",
    "        if type(b) != int or type(w) != int or type(min_) != float or type(max_) != float:\n",
    "            raise TypeError(\"b - buckets must be int, w - width must be int, min_ must be float and max_ must be float\")\n",
    "        self.b = b # must be int\n",
    "        self.w = w # must be int\n",
    "        self.min = min_ # must be float\n",
    "        self.max = max_ # must be float\n",
    "        self.n = b+w-1 # number of units for encoding\n",
    "        self.ndarray_shape = shape\n",
    "        \n",
    "    def encode(self,input_):\n",
    "        if input_ > self.max or input_ < self.min:\n",
    "            raise ValueError(\"Input outside encoder range!\")\n",
    "        if type(input_) != float:\n",
    "            raise TypeError(\"Input must be float!\")\n",
    "        output = np.zeros(self.n)-1\n",
    "        index = int((input_-self.min)/(self.max-self.min)*self.b)\n",
    "        output[index:index+self.w] = 1\n",
    "        return output\n",
    "    \n",
    "    def encode_ndarray(self,input_):\n",
    "        if input_.shape != self.ndarray_shape:\n",
    "            raise ValueError(\"Input dimensions do not match specified encoder dimensions!\")\n",
    "        output = []\n",
    "        for i in np.nditer(input_, order='K'):\n",
    "            output.append(self.encode(float(i)))\n",
    "        return np.array(output)\n",
    "    '''\n",
    "    def decode(self,input_):\n",
    "        if len(input_) != self.n: # or len(np.nonzero(input_+1)[0]) != self.w: <-- Can't have since the network is not guaranteed to produce this by any means!!!\n",
    "            raise TypeError(\"Input does not correspond to encoder encoded data!\")\n",
    "        # output = np.nonzero(input_+1)[0][0]/float(self.b)*(self.max-self.min)+self.min <-- This doesn't work really since bits can randomly fire, taking the average is a more reasonable decoding\n",
    "        median = np.median(np.nonzero(input_+1)[0])            \n",
    "        try:\n",
    "            output = int(median-float(self.w)/2.0)/float(self.b)*(self.max-self.min)+self.min # i.e. figure out center (median more outlier resistant than mean) and subtract width/2\n",
    "        except ValueError:\n",
    "            output = None\n",
    "        return output\n",
    "    '''\n",
    "    def decode(self,input_):\n",
    "        if len(input_) != self.n: # or len(np.nonzero(input_+1)[0]) != self.w: <-- Can't have since the network is not guaranteed to produce this by any means!!!\n",
    "            raise TypeError(\"Input does not correspond to encoder encoded data!\")\n",
    "        if len(np.nonzero(input_+1)[0]) == 0:\n",
    "            return np.nan\n",
    "        max_ = 0\n",
    "        output = 0.0\n",
    "        for i in range(self.b):\n",
    "            x = np.zeros(self.n)-1\n",
    "            x[i:i+self.w] = 1\n",
    "            score = np.sum(np.array(x)*input_)\n",
    "            if score > max_:\n",
    "                max_ = score\n",
    "                output = float(i)/float(self.b)*(self.max-self.min)+self.min\n",
    "        return output\n",
    "            \n",
    "    def decode_ndarray(self,input_):\n",
    "        if input_.shape != (reduce(lambda x, y: x*y, self.ndarray_shape)*self.n,): \n",
    "            raise ValueError(\"Input dimensions do not match specified encoder dimensions!\")\n",
    "        input_ = input_.reshape(self.ndarray_shape+(self.n,))\n",
    "        output = []\n",
    "        for i in np.ndindex(self.ndarray_shape):\n",
    "            output.append(self.decode(input_[i]))\n",
    "        output = np.array(output).reshape(self.ndarray_shape)\n",
    "        return output\n",
    "    \n",
    "    def set_ndarray_shape(self,shape):\n",
    "        if type(shape) != tuple:\n",
    "            raise TypeError(\"Must provide tuple of array dimensions!\")\n",
    "        self.ndarray_shape = shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matrix_encoder = scalar_sdr(60,21,-0.3,0.3,(6,12))\n",
    "#pos_encoder = scalar_sdr(500,21,-100000.0,100000.0,(6,))\n",
    "#vel_encoder = scalar_sdr(200,41,-70.0,70.0,(6,))\n",
    "#brain_encoder = scalar_sdr(1000,101,0.0,1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: \n",
    "# - add transition to zero\n",
    "# - cleanup data generation using more general sense of transition (including zero and auto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So what data do we need? What system output do we want? What are the conditions?\n",
    "# Overall input/output vectors (encoded): [state_matrix,matrix,motor_pos,motor_vel,brain_sig]\n",
    "# Note: \n",
    "#      state matrix -- a sensory value from the dep node\n",
    "#      motor_pos and motor_vel -- sensory values from motors\n",
    "#      brain_sig -- single scalar input from brain\n",
    "#      matrix -- the only output of the system, sending a DEP matrix to the dep node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 1: system stationary i.e. zeros (technically transition from zero)\n",
    "# Input: [state_matrix = 0, matrix = 0, motor_pos = 0, motor_vel = 0, brain_id = brain_id]\n",
    "# Desired output: [state_matrix = 0, matrix = matrices[brain_id], motor_pos = 0, motor_vel = 0, brain_sig = brain_id]\n",
    "# Training data = desired output for each brain_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "stationary_data = {}\n",
    "for id_ in brain_id:\n",
    "    stationary_data[id_] = np.array([])\n",
    "    \n",
    "    state_matrix = matrix_encoder.encode_ndarray(matrices[\"zero\"]) # Note: this is not changed since it is sensory\n",
    "    stationary_data[id_] = np.append(stationary_data[id_],state_matrix.flatten())\n",
    "    \n",
    "    matrix = matrix_encoder.encode_ndarray(matrices[id_])\n",
    "    stationary_data[id_] = np.append(stationary_data[id_], matrix.flatten())\n",
    "    \n",
    "    motor_pos = pos_encoder.encode_ndarray(pos[\"zero\"])\n",
    "    stationary_data[id_] = np.append(stationary_data[id_],motor_pos.flatten())\n",
    "    \n",
    "    motor_vel = vel_encoder.encode_ndarray(vel[\"zero\"])\n",
    "    stationary_data[id_] = np.append(stationary_data[id_],motor_vel.flatten())\n",
    "    \n",
    "    brain_sig = brain_encoder.encode(brain_id[id_])\n",
    "    stationary_data[id_] = np.append(stationary_data[id_],brain_sig.flatten())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 2: transition from one behavior to another based on the input brain_id at the correct motor_pos and motor_vel\n",
    "# Input: [state_matrix = state_matrix, matrix = 0, motor_pos = motor_pos, motor_vel = motor_vel, brain_sig = brain_id]\n",
    "\n",
    "# Desired output: if system state equals the transition point, transition, else don't i.e. requires \"positive\" and \"negative examples\"\n",
    "\n",
    "# if motor_pos ~= pos[brain_id[ and motor_vel ~= vel[brain_id]:\n",
    "#     state_matrix = state_matrix, matrix = matrix[brain_id], motor_pos = motor_pos, motor_vel = motor_vel, brain_sig = brain_id]\n",
    "# else:\n",
    "#    state_matrix = state_matrix, matrix = state_matrix, motor_pos = motor_pos, motor_vel = motor_vel, brain_sig = brain_id]\n",
    "\n",
    "# Training data: single positive example for each transition pair, lots of negatives examples for different motor_pos and motor_vel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transition point positive examples\n",
    "transitions_positive_data = {}\n",
    "for transition in transitions:\n",
    "    transitions_positive_data[transition] = []\n",
    "    \n",
    "    m0 = matrix_encoder.encode_ndarray(matrices[transition[0]])\n",
    "    transitions_positive_data[transition] = np.append(transitions_positive_data[transition], m0.flatten())\n",
    "    \n",
    "    m1 = matrix_encoder.encode_ndarray(matrices[transition[1]])\n",
    "    transitions_positive_data[transition] = np.append(transitions_positive_data[transition], m1.flatten())\n",
    "    \n",
    "    p = pos_encoder.encode_ndarray(pos[transition[0]])\n",
    "    transitions_positive_data[transition] = np.append(transitions_positive_data[transition], p.flatten())\n",
    "    \n",
    "    v = vel_encoder.encode_ndarray(vel[transition[0]])\n",
    "    transitions_positive_data[transition] = np.append(transitions_positive_data[transition], v.flatten())\n",
    "    \n",
    "    brain_sig = brain_encoder.encode(brain_id[transition[1]])\n",
    "    transitions_positive_data[transition] = np.append(transitions_positive_data[transition], brain_sig.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transition point negative examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup negative motor_pos and motor_vel data\n",
    "\n",
    "neg_pos = {\"fb\": [], \"fs\": [], \"sd\": []}\n",
    "neg_vel = {\"fb\": [], \"fs\": [], \"sd\": []}\n",
    "\n",
    "steps = [0, 12, 24, 36, 48, 60, 72, 84, 96, 108]\n",
    "\n",
    "for key in bases:\n",
    "    for i in steps:\n",
    "        neg_pos[key].append(bases[key][0][i][active_motors])\n",
    "        neg_vel[key].append(bases[key][1][i][active_motors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain negative examples\n",
    "transitions_negative_data = {}\n",
    "for transition in transitions:\n",
    "    transitions_negative_data[transition] = []\n",
    "    for i in range(len(neg_pos[transition[0]])):\n",
    "        temp = np.array([])\n",
    "        \n",
    "        m0 = matrix_encoder.encode_ndarray(matrices[transition[0]])\n",
    "        temp = np.append(temp, m0.flatten())\n",
    "\n",
    "        m1 = m0\n",
    "        temp = np.append(temp, m1.flatten())\n",
    "\n",
    "        p = pos_encoder.encode_ndarray(neg_pos[transition[0]][i])\n",
    "        temp = np.append(temp, p.flatten())\n",
    "\n",
    "        v = vel_encoder.encode_ndarray(neg_vel[transition[0]][i])\n",
    "        temp = np.append(temp, v.flatten())\n",
    "\n",
    "        brain_sig = brain_encoder.encode(brain_id[transition[1]])\n",
    "        temp = np.append(temp, brain_sig.flatten())\n",
    "        \n",
    "        transitions_negative_data[transition].append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 3: keep situation the same, active high brain signal (technically transitioning from self)\n",
    "# Input: [state_matrix = state_matrix[brain_id], matrix = 0, motor_pos = motor_pos, motor_vel = motor_vel, brain_sig = brain_id]\n",
    "# Desired output: [state_matrix = state_matrix[brain_id], matrix = matrix[brain_id], motor_pos = motor_pos, motor_vel = motor_vel, brain_sig = brain_id]\n",
    "# Training data: Desired output over motor_pos and motor_vel range\n",
    "\n",
    "maintain = {}\n",
    "for id_ in matrices:\n",
    "    if id_ != \"zero\":\n",
    "        maintain[id_] = []\n",
    "        for i in range(0,len(bases[id_][0]),12):\n",
    "            temp = np.array([])\n",
    "            m0 = matrix_encoder.encode_ndarray(matrices[id_])\n",
    "            temp = np.append(temp, m0.flatten())\n",
    "\n",
    "            m1 = m0\n",
    "            temp = np.append(temp, m1.flatten())\n",
    "\n",
    "            p = pos_encoder.encode_ndarray(bases[key][0][i][active_motors])\n",
    "            temp = np.append(temp, p.flatten())\n",
    "\n",
    "            v = vel_encoder.encode_ndarray(bases[key][1][i][active_motors])\n",
    "            temp = np.append(temp, v.flatten())\n",
    "\n",
    "            braing_sig = brain_encoder.encode(brain_id[id_])\n",
    "            temp = np.append(temp, braing_sig.flatten())\n",
    "            maintain[id_].append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 4: shutdown (transition to zero)\n",
    "# Input: [state_matrix = state_matrix, matrix = 0, motor_pos = motor_pos, motor_vel = motor_vel, brain_sig = zero]\n",
    "# Desired output: [state_matrix = state_matrox, matrix = 0, motor_pos = motor_pos, motor_vel = motor_vel, brain_sig = zero]\n",
    "# Training data: Desired output over motor_pos and motor_vel range\n",
    "\n",
    "shutdown = {}\n",
    "for id_ in matrices:\n",
    "    if id_ != \"zero\":\n",
    "        shutdown[id_] = []\n",
    "        for i in range(0,len(bases[id_][0]),12):\n",
    "            temp = np.array([])\n",
    "            m0 = matrix_encoder.encode_ndarray(matrices[id_])\n",
    "            temp = np.append(temp, m0.flatten())\n",
    "\n",
    "            m1 = matrix_encoder.encode_ndarray(matrices[\"zero\"])\n",
    "            temp = np.append(temp, m1.flatten())\n",
    "\n",
    "            p = pos_encoder.encode_ndarray(bases[key][0][i][active_motors])\n",
    "            temp = np.append(temp, p.flatten())\n",
    "\n",
    "            v = vel_encoder.encode_ndarray(bases[key][1][i][active_motors])\n",
    "            temp = np.append(temp, v.flatten())\n",
    "\n",
    "            braing_sig = brain_encoder.encode(brain_id[\"zero\"])\n",
    "            temp = np.append(temp, braing_sig.flatten())\n",
    "            shutdown[id_].append(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Hopfield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "hnn = Hopfield_Neural_Network(17180,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stationary data\n",
    "for item in stationary_data:\n",
    "    hnn.store(stationary_data[item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive transitions data\n",
    "for item in transitions_positive_data:\n",
    "    hnn.store(transitions_positive_data[item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative transitions\n",
    "for key in transitions_negative_data:\n",
    "    for item in transitions_negative_data[key]:\n",
    "        hnn.store(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maintain status\n",
    "for key in maintain:\n",
    "    for item in maintain[key]:\n",
    "        hnn.store(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shutdown data\n",
    "for key in shutdown:\n",
    "    for item in maintain[key]:\n",
    "        hnn.store(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save hnn weights\n",
    "hnn.save_weights('hnn_weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. \"Recall\" matrices from complete matrix\n",
    "### 1.1  Vary encoder buckets and width and investigate resulting error and processing times (store and recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buckets - 30, Width - 11\n",
      "------------------------------------------------------------------------------\n",
      "Id: fb\n",
      "Encoding time: 0.000568\n",
      "Storing time: 0.036327\n",
      "Id: fs\n",
      "Encoding time: 0.000614\n",
      "Storing time: 0.031066\n",
      "Id: zero\n",
      "Encoding time: 0.001033\n",
      "Storing time: 0.029890\n",
      "Id: sd\n",
      "Encoding time: 0.000639\n",
      "Storing time: 0.029311\n",
      "Recall time: 0.301398\n",
      "Decode time: 0.019025\n",
      "Mem decoded: \n",
      "[[ 0.083  0.05  -0.117  0.083 -0.117 -0.15  -0.183 -0.183  0.167 -0.183\n",
      "   0.183  0.183]\n",
      " [ 0.1    0.1    0.117 -0.133 -0.133  0.    -0.2   -0.167 -0.183  0.15\n",
      "   0.15   0.   ]\n",
      " [-0.1    0.067  0.1   -0.1    0.1    0.133  0.167 -0.167 -0.183  0.167\n",
      "  -0.2   -0.217]\n",
      " [ 0.083 -0.083 -0.117  0.083 -0.117 -0.15  -0.183  0.15   0.167 -0.183\n",
      "   0.183  0.183]\n",
      " [-0.1   -0.083  0.083 -0.083  0.083  0.133  0.167  0.15  -0.183  0.167\n",
      "  -0.2   -0.217]\n",
      " [-0.067  0.     0.067 -0.067  0.083  0.117  0.167  0.    -0.183  0.167\n",
      "  -0.217 -0.233]]\n",
      "Diff: \n",
      "[[ 0.     0.133 -0.017  0.     0.     0.     0.    -0.333  0.     0.     0.\n",
      "  -0.017]\n",
      " [ 0.217  0.     0.     0.    -0.267 -0.15  -0.35   0.    -0.017  0.     0.333\n",
      "   0.183]\n",
      " [ 0.     0.     0.017  0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   0.   ]\n",
      " [ 0.     0.    -0.017  0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  -0.017]\n",
      " [-0.017 -0.133  0.     0.     0.     0.     0.     0.317  0.     0.     0.017\n",
      "   0.   ]\n",
      " [ 0.    -0.033  0.     0.     0.     0.    -0.017  0.167  0.    -0.017  0.\n",
      "   0.   ]]\n",
      "Recall time: 0.338883\n",
      "Decode time: 0.020795\n",
      "Mem decoded: \n",
      "[[ 0.1    0.067 -0.117  0.083 -0.117 -0.15  -0.183 -0.183  0.167 -0.183\n",
      "   0.183  0.183]\n",
      " [ 0.1    0.1    0.117 -0.133 -0.133  0.    -0.183 -0.167 -0.183  0.15\n",
      "   0.15   0.   ]\n",
      " [-0.1    0.067  0.1   -0.1    0.1    0.133  0.167 -0.167 -0.183  0.167\n",
      "  -0.2   -0.217]\n",
      " [ 0.083 -0.083 -0.117  0.083 -0.117 -0.15  -0.183  0.15   0.167 -0.183\n",
      "   0.183  0.183]\n",
      " [-0.1   -0.083  0.083 -0.083  0.1    0.133  0.167  0.15  -0.183  0.167\n",
      "  -0.2   -0.217]\n",
      " [-0.067  0.     0.067 -0.067  0.083  0.117  0.167  0.    -0.183  0.167\n",
      "  -0.217 -0.233]]\n",
      "Diff: \n",
      "[[ 0.     0.     0.    -0.017  0.     0.     0.     0.     0.     0.     0.\n",
      "   0.   ]\n",
      " [ 0.     0.     0.25  -0.25   0.     0.167 -0.017  0.017 -0.333  0.333\n",
      "  -0.017 -0.167]\n",
      " [ 0.     0.15   0.     0.     0.     0.     0.    -0.333  0.     0.     0.\n",
      "  -0.017]\n",
      " [-0.017 -0.15   0.    -0.017  0.     0.     0.     0.333  0.     0.     0.\n",
      "   0.   ]\n",
      " [ 0.     0.    -0.017  0.017  0.     0.     0.    -0.017  0.     0.     0.\n",
      "  -0.017]\n",
      " [ 0.017  0.067 -0.017  0.017 -0.017  0.     0.    -0.167  0.     0.    -0.017\n",
      "  -0.017]]\n",
      "Recall time: 0.360772\n",
      "Decode time: 0.017217\n",
      "Mem decoded: \n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "Diff: \n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "Recall time: 0.305699\n",
      "Decode time: 0.018500\n",
      "Mem decoded: \n",
      "[[ 0.1    0.067  0.     0.    -0.117  0.    -0.2   -0.167  0.       nan\n",
      "   0.183  0.   ]\n",
      " [ 0.1    0.1    0.117 -0.1   -0.133  0.    -0.183 -0.167 -0.183  0.15\n",
      "   0.15   0.   ]\n",
      " [ 0.     0.067  0.1   -0.1    0.     0.     0.    -0.15  -0.183  0.167\n",
      "   0.167  0.   ]\n",
      " [ 0.    -0.083 -0.117  0.083  0.     0.     0.     0.15   0.167 -0.183\n",
      "     nan  0.   ]\n",
      " [-0.1   -0.083  0.     0.     0.1    0.     0.167  0.15   0.083  0.    -0.2\n",
      "   0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   0.   ]]\n",
      "Diff: \n",
      "[[ 0.     0.    -0.083  0.1    0.     0.     0.     0.     0.183    nan  0.\n",
      "   0.   ]\n",
      " [-0.017  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   0.   ]\n",
      " [-0.1   -0.017  0.     0.     0.117  0.     0.2    0.     0.     0.     0.\n",
      "   0.   ]\n",
      " [ 0.117  0.017  0.     0.    -0.1    0.    -0.167  0.     0.     0.       nan\n",
      "   0.   ]\n",
      " [ 0.017  0.     0.1   -0.083  0.     0.    -0.017  0.    -0.083  0.183\n",
      "  -0.017  0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   0.   ]]\n",
      "------------------------------------------------------------------------------\n",
      "Buckets - 60, Width - 11\n",
      "------------------------------------------------------------------------------\n",
      "Id: fb\n",
      "Encoding time: 0.000670\n",
      "Storing time: 0.097574\n",
      "Id: fs\n",
      "Encoding time: 0.000575\n",
      "Storing time: 0.097783\n",
      "Id: zero\n",
      "Encoding time: 0.000508\n",
      "Storing time: 0.097227\n",
      "Id: sd\n",
      "Encoding time: 0.000480\n",
      "Storing time: 0.105914\n",
      "Recall time: 0.531390\n",
      "Decode time: 0.033268\n",
      "Mem decoded: \n",
      "[[ 0.1    0.075 -0.117  0.092 -0.117 -0.15  -0.175 -0.175  0.167 -0.175\n",
      "   0.183  0.192]\n",
      " [ 0.108  0.1    0.117 -0.125 -0.125  0.117 -0.183 -0.167 -0.175  0.15\n",
      "   0.15  -0.192]\n",
      " [-0.1    0.067  0.1   -0.092  0.108  0.133  0.167 -0.167 -0.175  0.167\n",
      "  -0.2   -0.208]\n",
      " [ 0.092 -0.075 -0.108  0.092 -0.117 -0.15  -0.175  0.158  0.167 -0.175\n",
      "   0.183  0.2  ]\n",
      " [-0.1   -0.083  0.083 -0.092  0.1    0.133  0.175  0.158 -0.183  0.175\n",
      "  -0.208 -0.217]\n",
      " [-0.067  0.     0.067 -0.067  0.083  0.125  0.175 -0.242 -0.183  0.175\n",
      "  -0.217 -0.233]]\n",
      "Diff: \n",
      "[[ 0.017  0.158 -0.017  0.     0.     0.     0.    -0.333  0.     0.    -0.008\n",
      "  -0.008]\n",
      " [ 0.225  0.    -0.008  0.    -0.258 -0.042 -0.333  0.    -0.017  0.     0.333\n",
      "  -0.008]\n",
      " [-0.008  0.     0.008  0.     0.    -0.008 -0.008  0.     0.     0.     0.\n",
      "   0.   ]\n",
      " [ 0.     0.    -0.008  0.     0.     0.     0.     0.     0.     0.    -0.008\n",
      "   0.   ]\n",
      " [-0.017 -0.133  0.    -0.008  0.008  0.     0.     0.317  0.     0.     0.\n",
      "   0.   ]\n",
      " [ 0.    -0.042  0.     0.     0.     0.    -0.008 -0.083  0.    -0.008  0.\n",
      "   0.   ]]\n",
      "Recall time: 0.573093\n",
      "Decode time: 0.039296\n",
      "Mem decoded: \n",
      "[[ 0.1    0.075 -0.117  0.1   -0.117 -0.15  -0.175 -0.175  0.167 -0.175\n",
      "   0.183  0.192]\n",
      " [ 0.108  0.1    0.108 -0.125 -0.125 -0.225 -0.183 -0.167 -0.175  0.142\n",
      "   0.158  0.175]\n",
      " [-0.1    0.067  0.1   -0.092  0.108  0.133  0.167 -0.167 -0.175  0.167\n",
      "  -0.2   -0.208]\n",
      " [ 0.092 -0.092 -0.108  0.092 -0.117 -0.15  -0.175  0.158  0.167 -0.175\n",
      "   0.183  0.192]\n",
      " [-0.1   -0.083  0.083 -0.1    0.1    0.133  0.167  0.158 -0.183  0.175\n",
      "  -0.192 -0.208]\n",
      " [-0.067 -0.008  0.067 -0.067  0.083  0.125  0.175  0.175 -0.183  0.175\n",
      "  -0.217 -0.233]]\n",
      "Diff: \n",
      "[[ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   0.   ]\n",
      " [ 0.     0.     0.233 -0.25   0.    -0.067 -0.017  0.008 -0.333  0.317\n",
      "  -0.008  0.   ]\n",
      " [ 0.     0.15   0.     0.008  0.     0.     0.    -0.333  0.     0.    -0.008\n",
      "  -0.008]\n",
      " [-0.008 -0.167  0.008 -0.008  0.     0.     0.     0.333  0.     0.     0.\n",
      "   0.   ]\n",
      " [ 0.    -0.008 -0.017  0.     0.     0.     0.    -0.017 -0.008  0.     0.\n",
      "  -0.008]\n",
      " [ 0.017  0.058 -0.017  0.017 -0.017  0.     0.     0.     0.     0.    -0.017\n",
      "  -0.025]]\n",
      "Recall time: 0.618019\n",
      "Decode time: 0.031644\n",
      "Mem decoded: \n",
      "[[ 0.1    0.    -0.05   0.    -0.117  0.    -0.192 -0.175 -0.058 -0.042\n",
      "   0.183  0.   ]\n",
      " [-0.008  0.1    0.067 -0.042 -0.125  0.    -0.233 -0.175 -0.175  0.142\n",
      "   0.158  0.   ]\n",
      " [-0.025  0.067  0.092 -0.108    nan  0.    -0.008 -0.175 -0.183  0.167\n",
      "     nan  0.   ]\n",
      " [ 0.    -0.075 -0.125  0.092 -0.058  0.       nan  0.158  0.167 -0.175\n",
      "  -0.025  0.   ]\n",
      " [-0.083 -0.083  0.     0.     0.083  0.     0.167  0.158 -0.083 -0.05\n",
      "  -0.208  0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   0.   ]]\n",
      "Diff: \n",
      "[[ 0.1    0.    -0.05   0.    -0.117  0.    -0.192 -0.175 -0.058 -0.042\n",
      "   0.183  0.   ]\n",
      " [-0.008  0.1    0.067 -0.042 -0.125  0.    -0.233 -0.175 -0.175  0.142\n",
      "   0.158  0.   ]\n",
      " [-0.025  0.067  0.092 -0.108    nan  0.    -0.008 -0.175 -0.183  0.167\n",
      "     nan  0.   ]\n",
      " [ 0.    -0.075 -0.125  0.092 -0.058  0.       nan  0.158  0.167 -0.175\n",
      "  -0.025  0.   ]\n",
      " [-0.083 -0.083  0.     0.     0.083  0.     0.167  0.158 -0.083 -0.05\n",
      "  -0.208  0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   0.   ]]\n",
      "Recall time: 0.517709\n",
      "Decode time: 0.032065\n",
      "Mem decoded: \n",
      "[[ 0.1    0.075  0.042 -0.158 -0.117  0.    -0.192 -0.175 -0.183  0.167\n",
      "   0.183  0.   ]\n",
      " [ 0.108  0.1    0.117 -0.125 -0.125  0.    -0.183 -0.167 -0.175  0.15\n",
      "   0.158  0.   ]\n",
      " [   nan  0.067  0.1   -0.092    nan  0.    -0.25  -0.158 -0.183  0.167\n",
      "   0.15   0.   ]\n",
      " [-0.175 -0.092 -0.108  0.092  0.1    0.       nan  0.158  0.167 -0.175\n",
      "  -0.183  0.   ]\n",
      " [-0.108 -0.083 -0.167  0.05   0.1    0.     0.175  0.158  0.158 -0.2\n",
      "  -0.192  0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   0.   ]]\n",
      "Diff: \n",
      "[[ 0.     0.    -0.05  -0.067 -0.008  0.     0.    -0.017  0.    -0.017  0.\n",
      "   0.   ]\n",
      " [-0.017 -0.008  0.    -0.025  0.     0.     0.    -0.008  0.    -0.008  0.\n",
      "   0.   ]\n",
      " [   nan -0.017  0.     0.       nan  0.    -0.058 -0.008  0.    -0.008\n",
      "  -0.025  0.   ]\n",
      " [-0.058  0.     0.     0.    -0.008  0.       nan  0.     0.     0.     0.\n",
      "   0.   ]\n",
      " [ 0.     0.    -0.067 -0.033  0.     0.    -0.008  0.    -0.017 -0.017\n",
      "  -0.008  0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   0.   ]]\n",
      "------------------------------------------------------------------------------\n",
      "Buckets - 90, Width - 11\n",
      "------------------------------------------------------------------------------\n",
      "Id: fb\n",
      "Encoding time: 0.000585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing time: 0.196751\n",
      "Id: fs\n",
      "Encoding time: 0.000582\n",
      "Storing time: 0.177448\n",
      "Id: zero\n",
      "Encoding time: 0.000608\n",
      "Storing time: 0.181862\n",
      "Id: sd\n",
      "Encoding time: 0.000542\n",
      "Storing time: 0.186511\n",
      "Recall time: 0.818608\n",
      "Decode time: 0.057604\n",
      "Mem decoded: \n",
      "[[ 0.1    0.072 -0.1    0.089 -0.117 -0.15  -0.178 -0.172  0.167 -0.172\n",
      "   0.183  0.189]\n",
      " [ 0.111  0.106  0.117 -0.122 -0.122  0.133 -0.183 -0.161 -0.178  0.156\n",
      "   0.156 -0.2  ]\n",
      " [-0.094  0.067  0.094 -0.094  0.106  0.139  0.167 -0.161 -0.172  0.172\n",
      "  -0.2   -0.206]\n",
      " [ 0.089 -0.078 -0.106  0.089 -0.117 -0.15  -0.178  0.161  0.167 -0.172\n",
      "   0.183  0.2  ]\n",
      " [-0.094 -0.083  0.083 -0.1    0.1    0.133  0.178  0.156 -0.178  0.178\n",
      "  -0.206 -0.217]\n",
      " [-0.072  0.     0.067 -0.072  0.083  0.122  0.178 -0.161 -0.183  0.178\n",
      "  -0.211 -0.228]]\n",
      "Diff: \n",
      "[[ 0.011  0.15   0.     0.     0.     0.     0.    -0.333  0.     0.    -0.006\n",
      "  -0.011]\n",
      " [ 0.228  0.006 -0.006  0.    -0.261 -0.022 -0.339  0.    -0.017  0.     0.333\n",
      "  -0.022]\n",
      " [ 0.     0.     0.     0.     0.    -0.006 -0.006  0.     0.     0.     0.\n",
      "   0.   ]\n",
      " [ 0.     0.    -0.006  0.     0.     0.     0.     0.     0.     0.    -0.006\n",
      "   0.   ]\n",
      " [-0.011 -0.139  0.    -0.017  0.006  0.     0.     0.317  0.     0.     0.\n",
      "   0.   ]\n",
      " [-0.006 -0.039  0.    -0.006  0.     0.    -0.006 -0.006  0.    -0.006  0.\n",
      "   0.   ]]\n",
      "Recall time: 0.835995\n",
      "Decode time: 0.064934\n",
      "Mem decoded: \n",
      "[[ 0.1    0.078 -0.111  0.089 -0.117 -0.15  -0.178 -0.172  0.167 -0.172\n",
      "   0.183  0.189]\n",
      " [ 0.111  0.106  0.117 -0.128 -0.122 -0.172 -0.178 -0.161 -0.178  0.156\n",
      "   0.161  0.167]\n",
      " [-0.1    0.067  0.1   -0.1    0.106  0.139  0.167 -0.167 -0.172  0.172\n",
      "  -0.194 -0.206]\n",
      " [ 0.089 -0.089 -0.106  0.089 -0.117 -0.144 -0.178  0.161  0.167 -0.172\n",
      "   0.183  0.189]\n",
      " [-0.094 -0.083  0.089 -0.1    0.1    0.133  0.178  0.156 -0.178  0.172\n",
      "  -0.194 -0.2  ]\n",
      " [-0.083  0.     0.067 -0.083  0.083  0.122  0.178  0.178 -0.183  0.178\n",
      "  -0.206 -0.217]]\n",
      "Diff: \n",
      "[[ 0.     0.     0.    -0.011  0.    -0.006  0.     0.     0.     0.     0.\n",
      "   0.   ]\n",
      " [ 0.     0.     0.239 -0.25   0.    -0.017 -0.011  0.011 -0.333  0.328\n",
      "  -0.006 -0.006]\n",
      " [ 0.     0.144  0.     0.    -0.006  0.     0.    -0.339  0.     0.    -0.006\n",
      "  -0.006]\n",
      " [-0.011 -0.167  0.006 -0.017  0.     0.    -0.006  0.333  0.     0.     0.\n",
      "   0.   ]\n",
      " [ 0.    -0.006 -0.011  0.    -0.006  0.     0.006 -0.017 -0.006  0.     0.\n",
      "   0.   ]\n",
      " [ 0.     0.067 -0.017  0.    -0.017  0.     0.     0.    -0.006  0.    -0.011\n",
      "  -0.006]]\n",
      "Recall time: 0.806669\n",
      "Decode time: 0.048842\n",
      "Mem decoded: \n",
      "[[ 0.083 -0.006  0.    -0.017 -0.117  0.    -0.189 -0.017 -0.006 -0.022\n",
      "   0.183  0.   ]\n",
      " [ 0.     0.1   -0.006 -0.011  0.     0.    -0.017 -0.172  0.    -0.044\n",
      "     nan  0.   ]\n",
      " [-0.028 -0.006  0.094 -0.111    nan  0.     0.    -0.033 -0.183  0.172\n",
      "  -0.022  0.   ]\n",
      " [-0.011  0.    -0.117  0.089 -0.028  0.    -0.022  0.     0.167 -0.178  0.\n",
      "   0.   ]\n",
      " [-0.111  0.     0.     0.     0.094  0.     0.172 -0.011  0.    -0.006\n",
      "  -0.206  0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   0.   ]]\n",
      "Diff: \n",
      "[[ 0.083 -0.006  0.    -0.017 -0.117  0.    -0.189 -0.017 -0.006 -0.022\n",
      "   0.183  0.   ]\n",
      " [ 0.     0.1   -0.006 -0.011  0.     0.    -0.017 -0.172  0.    -0.044\n",
      "     nan  0.   ]\n",
      " [-0.028 -0.006  0.094 -0.111    nan  0.     0.    -0.033 -0.183  0.172\n",
      "  -0.022  0.   ]\n",
      " [-0.011  0.    -0.117  0.089 -0.028  0.    -0.022  0.     0.167 -0.178  0.\n",
      "   0.   ]\n",
      " [-0.111  0.     0.     0.     0.094  0.     0.172 -0.011  0.    -0.006\n",
      "  -0.206  0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   0.   ]]\n",
      "Recall time: 0.822692\n",
      "Decode time: 0.050045\n",
      "Mem decoded: \n",
      "[[ 0.1    0.078  0.072 -0.094 -0.106  0.    -0.189 -0.172 -0.183  0.183\n",
      "   0.183  0.   ]\n",
      " [ 0.128  0.106  0.117 -0.1   -0.122  0.    -0.183 -0.156 -0.178  0.156\n",
      "   0.161  0.   ]\n",
      " [ 0.061  0.067  0.1   -0.094 -0.117  0.    -0.228 -0.161 -0.183  0.172\n",
      "   0.167  0.   ]\n",
      " [-0.117 -0.089 -0.106  0.089  0.083  0.     0.122  0.161  0.172 -0.178\n",
      "  -0.194  0.   ]\n",
      " [-0.106 -0.083 -0.111  0.067  0.1    0.     0.178  0.156  0.178 -0.178\n",
      "  -0.194  0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   0.   ]]\n",
      "Diff: \n",
      "[[ 0.     0.    -0.022 -0.006  0.     0.     0.    -0.017  0.     0.     0.\n",
      "   0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.     0.    -0.006  0.\n",
      "   0.   ]\n",
      " [-0.044 -0.017  0.    -0.006 -0.011  0.    -0.039 -0.011  0.    -0.006\n",
      "  -0.011  0.   ]\n",
      " [-0.006  0.     0.     0.    -0.028  0.    -0.056  0.     0.     0.    -0.011\n",
      "   0.   ]\n",
      " [ 0.     0.    -0.017 -0.017  0.     0.    -0.006  0.     0.     0.    -0.011\n",
      "   0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "   0.   ]]\n",
      "------------------------------------------------------------------------------\n",
      "Buckets - 120, Width - 11\n",
      "------------------------------------------------------------------------------\n",
      "Id: fb\n",
      "Encoding time: 0.000884\n",
      "Storing time: 0.278726\n",
      "Id: fs\n",
      "Encoding time: 0.000778\n",
      "Storing time: 0.280005\n",
      "Id: zero\n",
      "Encoding time: 0.000738\n",
      "Storing time: 0.246094\n",
      "Id: sd\n",
      "Encoding time: 0.000505\n",
      "Storing time: 0.255480\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-132-9f4fea63631f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mmem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mrecall_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-39d02bf5101a>\u001b[0m in \u001b[0;36mrecall\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mupdate_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mupdate_sequence\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True)\n",
    "buckets = [30,60,90,120,150,180,210,240,270,300]\n",
    "widths = [11, 21, 31]\n",
    "experiments = {}\n",
    "for width in widths:\n",
    "    for bucket in buckets:\n",
    "        print \"Buckets - %i, Width - %i\" %(bucket,width)\n",
    "        print \"------------------------------------------------------------------------------\"\n",
    "        experiments[(bucket,width)] = {}\n",
    "        # initialize encoder\n",
    "        m_encoder = scalar_sdr(bucket,width,-0.25,0.25,(6,12))\n",
    "        # initialize hopfield network\n",
    "        nodes = m_encoder.n*reduce(lambda x, y: x*y, m_encoder.ndarray_shape)\n",
    "        hnn = Hopfield_Neural_Network(nodes,10000)\n",
    "        # store matrices in network\n",
    "        for id_ in matrices:\n",
    "            print(\"Id: %s\" %id_)\n",
    "            experiments[(bucket,width)][id_] = {}\n",
    "            \n",
    "            t0 = time.time()\n",
    "            matrix = m_encoder.encode_ndarray(matrices[id_]).flatten()\n",
    "            t1 = time.time()\n",
    "            encoding_time = t1-t0\n",
    "            experiments[(bucket,width)][id_][\"encoding_time\"] = encoding_time\n",
    "            print(\"Encoding time: %f\" %encoding_time)\n",
    "            \n",
    "            t0 = time.time()\n",
    "            hnn.store(matrix)\n",
    "            t1 = time.time()\n",
    "            store_time = t1-t0\n",
    "            experiments[(bucket,width)][id_][\"storing_time\"] = store_time\n",
    "            print(\"Storing time: %f\" %store_time)\n",
    "            \n",
    "            \n",
    "        # recall matrices and store data for evaluation\n",
    "        for id_ in matrices:\n",
    "            \n",
    "            t0 = time.time()\n",
    "            mem = hnn.recall(m_encoder.encode_ndarray(matrices[id_]).flatten())\n",
    "            t1 = time.time()\n",
    "            recall_time = t1-t0\n",
    "            experiments[(bucket,width)][id_][\"recall_time\"] = recall_time\n",
    "            print(\"Recall time: %f\" %recall_time)\n",
    "            \n",
    "            t0 = time.time()\n",
    "            mem_decoded = m_encoder.decode_ndarray(mem)\n",
    "            t1 = time.time()\n",
    "            decode_time = t1-t0\n",
    "            experiments[(bucket,width)][id_][\"decode_time\"] = recall_time\n",
    "            print(\"Decode time: %f\" %decode_time)\n",
    "            \n",
    "            experiments[(bucket,width)][id_][\"mem_decoded\"] = mem_decoded\n",
    "            print(\"Mem decoded: \")\n",
    "            print(np.array(mem_decoded))\n",
    "            \n",
    "            matrix_decoded = m_encoder.decode_ndarray(m_encoder.encode_ndarray(matrices[id_]).flatten())\n",
    "            diff = mem_decoded-matrix_decoded\n",
    "            experiments[(bucket,width)][id_][\"diff\"] = diff\n",
    "            print(\"Diff: \")\n",
    "            print(np.array(diff))\n",
    "            \n",
    "        print \"------------------------------------------------------------------------------\"\n",
    "        \n",
    "print \"pickling experiment data\"\n",
    "pickle_out = open(\"experiments1.pickle\",\"wb\")\n",
    "pickle.dump(experiments, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conclusion: system minima exist at stored memories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Recall matrices from partial matrix\n",
    "\n",
    "    - Vary matrix portion that is provided to the input and identify systems capacity to complete the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buckets - 300, Width - 31\n",
      "------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-133-c68be03350c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mid_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmatrices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mmatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mhnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mportions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m95\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m90\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m85\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m75\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m70\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m65\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m55\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m45\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-39d02bf5101a>\u001b[0m in \u001b[0;36mstore\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mdW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mouter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_diagonal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True)\n",
    "experiments = {}\n",
    "buckets = [300]\n",
    "widths = [31]\n",
    "for width in widths:\n",
    "    for bucket in buckets:\n",
    "        print \"Buckets - %i, Width - %i\" %(bucket,width)\n",
    "        print \"------------------------------------------------------------------------------\"\n",
    "        experiments[(bucket,width)] = {}\n",
    "        m_encoder = scalar_sdr(bucket,width,-0.25,0.25,(6,12))\n",
    "        nodes = m_encoder.n*reduce(lambda x, y: x*y, m_encoder.ndarray_shape)\n",
    "        hnn = Hopfield_Neural_Network(nodes,200)\n",
    "\n",
    "        for id_ in matrices:\n",
    "            matrix = m_encoder.encode_ndarray(matrices[id_]).flatten()\n",
    "            hnn.store(matrix)\n",
    " \n",
    "        portions = [95,90,85,80,75,70,65,60,55,50,45,40]\n",
    "        for portion in portions:\n",
    "            experiments[(bucket,width)][portion] = {}\n",
    "            print \"Portion - %f\" %(portion)\n",
    "            for id_ in matrices:\n",
    "                experiments[(bucket,width)][portion][id_] = {}\n",
    "                print(\"Id: %s\" %id_)\n",
    "                matrix = m_encoder.encode_ndarray(matrices[id_]).flatten()\n",
    "                size = int(float(portion)/float(100)*len(matrix))\n",
    "                flip_indices = np.random.choice(a=hnn.nodes,size=size,replace=False)\n",
    "                print \"number of flip_indices %i\" %len(flip_indices)\n",
    "                matrix[flip_indices] = -1\n",
    "\n",
    "                mem = hnn.recall(matrix)\n",
    "\n",
    "                mem_decoded = m_encoder.decode_ndarray(mem)\n",
    "                #print(\"Mem decoded:\")\n",
    "                #print(mem_decoded)\n",
    "                matrix_decoded = m_encoder.decode_ndarray(m_encoder.encode_ndarray(matrices[id_]).flatten())\n",
    "                #print(\"Input matrix decoded: \")\n",
    "                #print(matrix_decoded)\n",
    "                diff = mem_decoded-matrix_decoded\n",
    "                print(\"Diff:\")\n",
    "                print(diff)\n",
    "                experiments[(bucket,width)][portion][id_][\"diff\"] = diff\n",
    "                print(\"Largest error: \"+str(np.max(abs(diff))))\n",
    "                experiments[(bucket,width)][portion][id_][\"max_error\"] = np.max(abs(diff))\n",
    "                print(\"Number of indices with error above 0.01: %i\" %len(abs(diff)[abs(diff) > 0.01]))\n",
    "                experiments[(bucket,width)][portion][id_][\"num_errors\"] = len(abs(diff)[abs(diff) > 0.01])\n",
    "        print \"------------------------------------------------------------------------------\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print \"pickling experiment data\"\n",
    "#pickle_out = open(\"experiments_1_2_portion_B300.pickle\",\"wb\")\n",
    "#pickle.dump(experiments, pickle_out)\n",
    "#pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts = {}\n",
    "portions = [95,90,85,80,75,70,65,60,55,50,45,40]\n",
    "for (bucket,width) in experiments:\n",
    "    portion_vs_max = []\n",
    "    portion_vs_num = []\n",
    "    for portion in portions:\n",
    "        max_ = 0.0\n",
    "        num_ = 0\n",
    "        for id_ in experiments[(bucket,width)][portion]:\n",
    "            if experiments[(bucket,width)][portion][id_][\"max_error\"] > max_:\n",
    "                max_ = experiments[(bucket,width)][portion][id_][\"max_error\"]\n",
    "            if experiments[(bucket,width)][portion][id_][\"num_errors\"] > num_:\n",
    "                num_ = experiments[(bucket,width)][portion][id_][\"num_errors\"]\n",
    "        portion_vs_max.append(max_)\n",
    "        portion_vs_num.append(num_)\n",
    "    dicts[(bucket,width)] = []\n",
    "    dicts[(bucket,width)].append(portion_vs_max)\n",
    "    dicts[(bucket,width)].append(portion_vs_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(240,\n",
       "  21): [[0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.029166666666666674,\n",
       "   0.033333333333333354,\n",
       "   0.025000000000000022,\n",
       "   0.03125,\n",
       "   0.022916666666666669,\n",
       "   0.018749999999999989,\n",
       "   0.020833333333333315,\n",
       "   0.025000000000000022,\n",
       "   0.018749999999999989], [39, 34, 33, 25, 21, 14, 12, 8, 9, 4, 2, 2]]}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the largest acceptable deviation such that DEP still performs the behavior?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Vary iterations to inspect impact on error and recall time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fb 0.00714285714286\n",
      "fs 0.00952380952381\n",
      "zero 0.111904761905\n",
      "sd 0.0142857142857\n",
      "\n",
      "fb 0.0104166666667\n",
      "fs 0.00625\n",
      "zero 0.00833333333333\n",
      "sd 0.00625\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xs = [(210,21),(240,21)]\n",
    "for x in xs:\n",
    "    for key in experiments[x]:\n",
    "        print key, np.max(abs(experiments[x][key][\"diff\"]))\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - Using buckets = 240, width = 21:\n",
    "        - 240 buckets is the first size to producing maximum errors of <= 0.01 (rounded to 3 sig.fig.) with 10000 \n",
    "        iterations for recalling from self, \n",
    "        - and width 21 to have more robustness against noise (as per suggestion of htm encoding paper) \n",
    "    - Note: later on it may be usefull to consider a larger width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations - 1\n",
      "------------------------------------------------------------------------------\n",
      "Id: fb\n",
      "Recall time: 0.000674\n",
      "Decode time: 0.168366\n",
      "Largest error: 0.0375\n",
      "Number of indices with error above 0.01: 9\n",
      "Id: fs\n",
      "Recall time: 0.000656\n",
      "Decode time: 0.165562\n",
      "Largest error: 0.0229166666667\n",
      "Number of indices with error above 0.01: 10\n",
      "Id: zero\n",
      "Recall time: 0.000729\n",
      "Decode time: 0.177017\n",
      "Largest error: 0.0270833333333\n",
      "Number of indices with error above 0.01: 11\n",
      "Id: sd\n",
      "Recall time: 0.001262\n",
      "Decode time: 0.184766\n",
      "Largest error: 0.0270833333333\n",
      "Number of indices with error above 0.01: 13\n",
      "------------------------------------------------------------------------------\n",
      "Iterations - 2\n",
      "------------------------------------------------------------------------------\n",
      "Id: fb\n",
      "Recall time: 0.001253\n",
      "Decode time: 0.168449\n",
      "Largest error: 0.0208333333333\n",
      "Number of indices with error above 0.01: 9\n",
      "Id: fs\n",
      "Recall time: 0.001371\n",
      "Decode time: 0.171695\n",
      "Largest error: 0.0291666666667\n",
      "Number of indices with error above 0.01: 13\n",
      "Id: zero\n",
      "Recall time: 0.001226\n",
      "Decode time: 0.164899\n",
      "Largest error: 0.025\n",
      "Number of indices with error above 0.01: 17\n",
      "Id: sd\n",
      "Recall time: 0.001314\n",
      "Decode time: 0.177933\n",
      "Largest error: 0.0416666666667\n",
      "Number of indices with error above 0.01: 9\n",
      "------------------------------------------------------------------------------\n",
      "Iterations - 5\n",
      "------------------------------------------------------------------------------\n",
      "Id: fb\n",
      "Recall time: 0.002709\n",
      "Decode time: 0.174533\n",
      "Largest error: 0.0208333333333\n",
      "Number of indices with error above 0.01: 10\n",
      "Id: fs\n",
      "Recall time: 0.002835\n",
      "Decode time: 0.167946\n",
      "Largest error: 0.0270833333333\n",
      "Number of indices with error above 0.01: 13\n",
      "Id: zero\n",
      "Recall time: 0.002871\n",
      "Decode time: 0.167428\n",
      "Largest error: 0.0291666666667\n",
      "Number of indices with error above 0.01: 11\n",
      "Id: sd\n",
      "Recall time: 0.002695\n",
      "Decode time: 0.182589\n",
      "Largest error: 0.01875\n",
      "Number of indices with error above 0.01: 11\n",
      "------------------------------------------------------------------------------\n",
      "Iterations - 10\n",
      "------------------------------------------------------------------------------\n",
      "Id: fb\n",
      "Recall time: 0.005483\n",
      "Decode time: 0.165827\n",
      "Largest error: 0.025\n",
      "Number of indices with error above 0.01: 11\n",
      "Id: fs\n",
      "Recall time: 0.005217\n",
      "Decode time: 0.176730\n",
      "Largest error: 0.0229166666667\n",
      "Number of indices with error above 0.01: 14\n",
      "Id: zero\n",
      "Recall time: 0.007952\n",
      "Decode time: 0.169062\n",
      "Largest error: 0.0270833333333\n",
      "Number of indices with error above 0.01: 13\n",
      "Id: sd\n",
      "Recall time: 0.005440\n",
      "Decode time: 0.164339\n",
      "Largest error: 0.025\n",
      "Number of indices with error above 0.01: 12\n",
      "------------------------------------------------------------------------------\n",
      "Iterations - 20\n",
      "------------------------------------------------------------------------------\n",
      "Id: fb\n",
      "Recall time: 0.010117\n",
      "Decode time: 0.164209\n",
      "Largest error: 0.0416666666667\n",
      "Number of indices with error above 0.01: 11\n",
      "Id: fs\n",
      "Recall time: 0.011555\n",
      "Decode time: 0.171448\n",
      "Largest error: 0.025\n",
      "Number of indices with error above 0.01: 11\n",
      "Id: zero\n",
      "Recall time: 0.009830\n",
      "Decode time: 0.163003\n",
      "Largest error: 0.0333333333333\n",
      "Number of indices with error above 0.01: 16\n",
      "Id: sd\n",
      "Recall time: 0.010363\n",
      "Decode time: 0.163511\n",
      "Largest error: 0.0333333333333\n",
      "Number of indices with error above 0.01: 15\n",
      "------------------------------------------------------------------------------\n",
      "Iterations - 50\n",
      "------------------------------------------------------------------------------\n",
      "Id: fb\n",
      "Recall time: 0.024157\n",
      "Decode time: 0.177325\n",
      "Largest error: 0.025\n",
      "Number of indices with error above 0.01: 12\n",
      "Id: fs\n",
      "Recall time: 0.031101\n",
      "Decode time: 0.162790\n",
      "Largest error: 0.0208333333333\n",
      "Number of indices with error above 0.01: 11\n",
      "Id: zero\n",
      "Recall time: 0.036562\n",
      "Decode time: 0.188059\n",
      "Largest error: 0.0270833333333\n",
      "Number of indices with error above 0.01: 14\n",
      "Id: sd\n",
      "Recall time: 0.047600\n",
      "Decode time: 0.219735\n",
      "Largest error: 0.0145833333333\n",
      "Number of indices with error above 0.01: 5\n",
      "------------------------------------------------------------------------------\n",
      "Iterations - 100\n",
      "------------------------------------------------------------------------------\n",
      "Id: fb\n",
      "Recall time: 0.059759\n",
      "Decode time: 0.177981\n",
      "Largest error: 0.0208333333333\n",
      "Number of indices with error above 0.01: 12\n",
      "Id: fs\n",
      "Recall time: 0.059968\n",
      "Decode time: 0.172178\n",
      "Largest error: 0.0333333333333\n",
      "Number of indices with error above 0.01: 8\n",
      "Id: zero\n",
      "Recall time: 0.063033\n",
      "Decode time: 0.170009\n",
      "Largest error: 0.0208333333333\n",
      "Number of indices with error above 0.01: 14\n",
      "Id: sd\n",
      "Recall time: 0.054690\n",
      "Decode time: 0.169322\n",
      "Largest error: 0.0333333333333\n",
      "Number of indices with error above 0.01: 14\n",
      "------------------------------------------------------------------------------\n",
      "Iterations - 200\n",
      "------------------------------------------------------------------------------\n",
      "Id: fb\n",
      "Recall time: 0.104859\n",
      "Decode time: 0.167044\n",
      "Largest error: 0.0291666666667\n",
      "Number of indices with error above 0.01: 8\n",
      "Id: fs\n",
      "Recall time: 0.087846\n",
      "Decode time: 0.167686\n",
      "Largest error: 0.0291666666667\n",
      "Number of indices with error above 0.01: 8\n",
      "Id: zero\n",
      "Recall time: 0.100753\n",
      "Decode time: 0.163403\n",
      "Largest error: 0.0166666666667\n",
      "Number of indices with error above 0.01: 10\n",
      "Id: sd\n",
      "Recall time: 0.087965\n",
      "Decode time: 0.168467\n",
      "Largest error: 0.0395833333333\n",
      "Number of indices with error above 0.01: 11\n",
      "------------------------------------------------------------------------------\n",
      "Iterations - 500\n",
      "------------------------------------------------------------------------------\n",
      "Id: fb\n",
      "Recall time: 0.299437\n",
      "Decode time: 0.170836\n",
      "Largest error: 0.0333333333333\n",
      "Number of indices with error above 0.01: 14\n",
      "Id: fs\n",
      "Recall time: 0.277636\n",
      "Decode time: 0.196213\n",
      "Largest error: 0.0375\n",
      "Number of indices with error above 0.01: 11\n",
      "Id: zero\n",
      "Recall time: 0.275310\n",
      "Decode time: 0.185011\n",
      "Largest error: 0.0270833333333\n",
      "Number of indices with error above 0.01: 15\n",
      "Id: sd\n",
      "Recall time: 0.271737\n",
      "Decode time: 0.177322\n",
      "Largest error: 0.0291666666667\n",
      "Number of indices with error above 0.01: 10\n",
      "------------------------------------------------------------------------------\n",
      "Iterations - 1000\n",
      "------------------------------------------------------------------------------\n",
      "Id: fb\n",
      "Recall time: 0.592906\n",
      "Decode time: 0.168905\n",
      "Largest error: 0.0270833333333\n",
      "Number of indices with error above 0.01: 13\n",
      "Id: fs\n",
      "Recall time: 0.418634\n",
      "Decode time: 0.162866\n",
      "Largest error: 0.0145833333333\n",
      "Number of indices with error above 0.01: 10\n",
      "Id: zero\n",
      "Recall time: 0.471671\n",
      "Decode time: 0.171544\n",
      "Largest error: 0.0270833333333\n",
      "Number of indices with error above 0.01: 6\n",
      "Id: sd\n",
      "Recall time: 0.585980\n",
      "Decode time: 0.172532\n",
      "Largest error: 0.0270833333333\n",
      "Number of indices with error above 0.01: 18\n",
      "------------------------------------------------------------------------------\n",
      "Iterations - 2000\n",
      "------------------------------------------------------------------------------\n",
      "Id: fb\n",
      "Recall time: 0.860490\n",
      "Decode time: 0.163563\n",
      "Largest error: 0.0166666666667\n",
      "Number of indices with error above 0.01: 8\n",
      "Id: fs\n",
      "Recall time: 0.963070\n",
      "Decode time: 0.165094\n",
      "Largest error: nan\n",
      "Number of indices with error above 0.01: 2\n",
      "Id: zero\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:53: RuntimeWarning: invalid value encountered in greater\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall time: 1.041961\n",
      "Decode time: 0.175099\n",
      "Largest error: 0.1625\n",
      "Number of indices with error above 0.01: 17\n",
      "Id: sd\n",
      "Recall time: 0.939770\n",
      "Decode time: 0.165462\n",
      "Largest error: 0.01875\n",
      "Number of indices with error above 0.01: 6\n",
      "------------------------------------------------------------------------------\n",
      "Iterations - 5000\n",
      "------------------------------------------------------------------------------\n",
      "Id: fb\n",
      "Recall time: 2.008793\n",
      "Decode time: 0.164669\n",
      "Largest error: 0.3375\n",
      "Number of indices with error above 0.01: 8\n",
      "Id: fs\n",
      "Recall time: 2.552482\n",
      "Decode time: 0.162128\n",
      "Largest error: 0.0166666666667\n",
      "Number of indices with error above 0.01: 8\n",
      "Id: zero\n",
      "Recall time: 2.528401\n",
      "Decode time: 0.163705\n",
      "Largest error: 0.166666666667\n",
      "Number of indices with error above 0.01: 21\n",
      "Id: sd\n",
      "Recall time: 2.153947\n",
      "Decode time: 0.175523\n",
      "Largest error: 0.0270833333333\n",
      "Number of indices with error above 0.01: 9\n",
      "------------------------------------------------------------------------------\n",
      "Iterations - 10000\n",
      "------------------------------------------------------------------------------\n",
      "Id: fb\n",
      "Recall time: 4.444355\n",
      "Decode time: 0.183152\n",
      "Largest error: 0.35\n",
      "Number of indices with error above 0.01: 9\n",
      "Id: fs\n",
      "Recall time: 6.127452\n",
      "Decode time: 0.190413\n",
      "Largest error: 0.3375\n",
      "Number of indices with error above 0.01: 12\n",
      "Id: zero\n",
      "Recall time: 7.207221\n",
      "Decode time: 0.161289\n",
      "Largest error: 0.18125\n",
      "Number of indices with error above 0.01: 23\n",
      "Id: sd\n",
      "Recall time: 4.175722\n",
      "Decode time: 0.159933\n",
      "Largest error: 0.0375\n",
      "Number of indices with error above 0.01: 11\n",
      "------------------------------------------------------------------------------\n",
      "pickling experiment data\n"
     ]
    }
   ],
   "source": [
    "iterations = [1, 2, 5, 10, 20, 50, 100, 200, 500, 1000, 2000, 5000, 10000]\n",
    "m_encoder = scalar_sdr(240,21,-0.25,0.25,(6,12))\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "experiment_iterations = {}\n",
    "portion = 70\n",
    "nodes = m_encoder.n*reduce(lambda x, y: x*y, m_encoder.ndarray_shape)\n",
    "hnn = Hopfield_Neural_Network(nodes)\n",
    "# store matrices\n",
    "for id_ in matrices:\n",
    "    matrix = m_encoder.encode_ndarray(matrices[id_]).flatten()\n",
    "    hnn.store(matrix)\n",
    "for iter_ in iterations:\n",
    "    print(\"Iterations - %i\" %iter_)\n",
    "    print \"------------------------------------------------------------------------------\"\n",
    "    hnn.setIter(iter_)\n",
    "    experiment_iterations[iter_] = {}\n",
    "    # recall matrices and store data for evaluation\n",
    "    for id_ in matrices:\n",
    "        experiment_iterations[iter_][id_] = {}\n",
    "        print(\"Id: %s\" %id_)\n",
    "        \n",
    "        matrix = m_encoder.encode_ndarray(matrices[id_]).flatten()\n",
    "        size = int(float(portion)/float(100)*len(matrix))\n",
    "        flip_indices = np.random.choice(a=hnn.nodes,size=size,replace=False)\n",
    "        matrix[flip_indices] = -1\n",
    "\n",
    "        t0 = time.time()\n",
    "        mem = hnn.recall(matrix)\n",
    "        t1 = time.time()\n",
    "        recall_time = t1-t0\n",
    "        experiment_iterations[iter_][id_][\"recall_time\"] = recall_time\n",
    "        print(\"Recall time: %f\" %recall_time)\n",
    "\n",
    "        t0 = time.time()\n",
    "        mem_decoded = m_encoder.decode_ndarray(mem)\n",
    "        t1 = time.time()\n",
    "        decode_time = t1-t0\n",
    "        experiment_iterations[iter_][id_][\"decode_time\"] = recall_time\n",
    "        print(\"Decode time: %f\" %decode_time)\n",
    "\n",
    "        experiment_iterations[iter_][id_][\"mem_decoded\"] = mem_decoded\n",
    "        #print(\"Mem decoded: \")\n",
    "        #print(np.array(mem_decoded))\n",
    "\n",
    "        matrix_decoded = m_encoder.decode_ndarray(m_encoder.encode_ndarray(matrices[id_]).flatten())\n",
    "        diff = mem_decoded-matrix_decoded\n",
    "\n",
    "        experiment_iterations[iter_][id_][\"diff\"] = diff\n",
    "        print(\"Largest error: \"+str(np.max(abs(diff))))\n",
    "        print(\"Number of indices with error above 0.01: %i\" %len(abs(diff)[abs(diff) > 0.01]))\n",
    "        #print(\"Diff: \")\n",
    "        #print(np.array(diff))\n",
    "        \n",
    "    print \"------------------------------------------------------------------------------\"\n",
    "print \"pickling experiment data\"\n",
    "pickle_out = open(\"experiment_iterations.pickle\",\"wb\")\n",
    "pickle.dump(experiment_iterations, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Recall matrix from brain id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Recall single matrix value from brain_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1707,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty good!\n",
    "# number of cases where error < 0.0125 â‰ƒ 280 (out of 288)\n",
    "# m_encoder = scalar_sdr(40,22,-0.25,0.25,matrix_shape)\n",
    "# b_encoder = scalar_sdr(44,20,0.0,1.0)\n",
    "# iter_ = 300\n",
    "# duration: 0.000701740673847 i.e. 0.7 ms * 72 = 50.5 ms (on my laptop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1708,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty good, but perhaps too slow?\n",
    "# number of cases where error < 0.00625 â‰ƒ 274 (out of 288)\n",
    "# m_encoder = scalar_sdr(80,44,-0.25,0.25,matrix_shape)\n",
    "# b_encoder = scalar_sdr(88,40,0.0,1.0)\n",
    "# iter_ = 600\n",
    "# duration:  0.00151696246531 i.e. 1.5 ms * 72 = 109 ms (on my laptop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1699,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_shape = (1,)\n",
    "# m_encoder = scalar_sdr(80,44,-0.25,0.25,matrix_shape)\n",
    "# b_encoder = scalar_sdr(88,40,0.0,1.0)\n",
    "m_encoder = scalar_sdr(40,22,-0.25,0.25,matrix_shape)\n",
    "b_encoder = scalar_sdr(44,20,0.0,1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1700,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = m_encoder.n*reduce(lambda x, y: x*y, m_encoder.ndarray_shape) + b_encoder.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1701,
   "metadata": {},
   "outputs": [],
   "source": [
    "nns = {}\n",
    "for index in np.ndindex(matrices[\"zero\"].shape):\n",
    "    hnn = Hopfield_Neural_Network(nodes)\n",
    "    for id_ in brain_id:\n",
    "        data = np.array([])\n",
    "        matrix = matrices[id_][index].reshape(matrix_shape)\n",
    "        matrix = m_encoder.encode_ndarray(matrix)\n",
    "        data = np.append(data,matrix.flatten())\n",
    "\n",
    "        brain_sig = brain_id[id_]\n",
    "        brain_sig = b_encoder.encode(brain_sig)\n",
    "        data = np.append(data,brain_sig)\n",
    "\n",
    "        hnn.store(data)\n",
    "        #print(\"%s\" %id_)\n",
    "        #print(\"Matrix decoded: \")\n",
    "        #print(m_encoder.decode_ndarray(m_encoder.encode_ndarray(matrices[id_][0,0].reshape(matrix_shape)).flatten()))\n",
    "\n",
    "        #print(\"Brain_sig decoded: \")\n",
    "        #print(b_encoder.decode(b_encoder.encode(brain_id[id_])))\n",
    "\n",
    "    nns[index] = hnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1702,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1703,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bargraph(diffs,max_,bars):\n",
    "    diffs = abs(np.array(diffs))\n",
    "    categories = []\n",
    "    values = np.zeros(bars)\n",
    "    #max_ = np.max(diffs)\n",
    "    for i in range(bars):\n",
    "        categories.append(i*max_/bars)\n",
    "    for diff in diffs:\n",
    "        i = int(diff/(max_+0.0000001)*bars)\n",
    "        values[i] += 1\n",
    "    return categories, values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1705,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nans:  0\n",
      "mean:  0.00252170138889\n",
      "max:  0.15\n",
      "std:  0.00855175236152\n",
      "duration:  0.000701740673847\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEACAYAAACwB81wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0a6d63da50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values[0]:  280.4\n"
     ]
    }
   ],
   "source": [
    "nans_avg = []\n",
    "means_avg = []\n",
    "maxs = []\n",
    "stds = []\n",
    "diffss = []\n",
    "durations = []\n",
    "for i in range(20):\n",
    "    nans = 0\n",
    "    diffs_m = []\n",
    "    for index in np.ndindex(matrices[\"zero\"].shape):\n",
    "        #print(\"Index: \"+str(index))\n",
    "        for id_ in brain_id:\n",
    "            #print id_\n",
    "            data = np.zeros(nodes)-1\n",
    "            brain_sig = brain_id[id_]\n",
    "            #print(brain_sig)\n",
    "            brain_sig = b_encoder.encode(brain_sig)\n",
    "            data[-b_encoder.n:] = brain_sig\n",
    "\n",
    "            nns[index].setIter(300)\n",
    "            t0 = time.time()\n",
    "            mem = nns[index].recall(data)\n",
    "            duration = time.time()-t0\n",
    "            durations.append(duration)\n",
    "            #print(\"Time: %f\" %duration)\n",
    "            matrix_out = mem[0:m_encoder.n*reduce(lambda x, y: x*y, m_encoder.ndarray_shape)]\n",
    "\n",
    "            matrix_decoded = m_encoder.decode_ndarray(matrix_out)\n",
    "            #print(\"Matrix decoded:\")\n",
    "            #print(matrix_decoded)\n",
    "            diff_m = m_encoder.decode_ndarray(m_encoder.encode_ndarray(matrices[id_][index].reshape(matrix_shape)).flatten())-matrix_decoded\n",
    "            if np.isnan(diff_m):\n",
    "                nans += 1\n",
    "            diffs_m.append(diff_m)\n",
    "            #print(\"diff_m: \")\n",
    "            #print \"Id: %s, Diff:\" %id_, diff_m[0]\n",
    "\n",
    "            brain_sig_out = mem[-b_encoder.n:]\n",
    "            brain_sig_decoded = b_encoder.decode(brain_sig_out)\n",
    "            #print(\"brain_sig decoded: \")\n",
    "            #print(brain_sig_decoded)\n",
    "            diff_b = b_encoder.decode(b_encoder.encode(brain_id[id_])) - brain_sig_decoded\n",
    "            #print(\"diff_b: \")\n",
    "            #print(diff_b)\n",
    "            #print\n",
    "#    print \"nans: \", nans\n",
    "#    print \"mean: \", np.mean(abs(np.array(diffs_m)))\n",
    "#    print \"max: \", np.max(abs(np.array(diffs_m)))\n",
    "#    print \"std: \", np.std(abs(np.array(diffs_m)))\n",
    "#    categories,values = bargraph(diffs_m,0.5,20)\n",
    "    \n",
    "    nans_avg.append(nans)\n",
    "    means_avg.append(np.mean(abs(np.array(diffs_m))))\n",
    "    maxs.append(np.max(abs(np.array(diffs_m))))\n",
    "    stds.append(np.std(abs(np.array(diffs_m))))\n",
    "    diffss.append(diffs_m)\n",
    "print \"nans: \", np.max(nans_avg)\n",
    "print \"mean: \", np.mean(means_avg)\n",
    "print \"max: \", np.max(maxs)\n",
    "print \"std: \", np.mean(stds)\n",
    "print \"duration: \", np.mean(durations)\n",
    "v = []\n",
    "for diff in diffss:\n",
    "    categories,values = bargraph(diff,0.5,40)\n",
    "    v.append(values)\n",
    "plt.figure(1)\n",
    "width = categories[1]\n",
    "v_avg = np.mean(v,axis=0)\n",
    "plt.bar(categories, np.mean(v,axis=0), width, align='center')\n",
    "plt.show()\n",
    "print \"Values[0]: \", v_avg[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
