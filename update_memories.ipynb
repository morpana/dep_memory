{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True)\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pickle\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Matrices\n",
    "class matrix_expansion:\n",
    "    \n",
    "    def __init__(self,active_motors):\n",
    "        self.active_motors = active_motors\n",
    "        active_sensors = np.array(active_motors*2)\n",
    "        active_sensors[len(active_motors):] += 14\n",
    "        self.active_sensors = active_sensors\n",
    "        self.shape = ()\n",
    "        \n",
    "    def load_from_file(self,filename):\n",
    "        f = open(filename,\"r\")\n",
    "        matrix = f.read()\n",
    "        f.close()\n",
    "        matrix = re.split(\",NEW_ROW,\",matrix)\n",
    "        matrix.pop()\n",
    "        matrix = np.array([np.array(re.split(\",\", row)).astype(np.float) for row in matrix])\n",
    "        self.shape = matrix.shape\n",
    "        return matrix\n",
    "        \n",
    "    def reduced_matrix(self,matrix):\n",
    "        matrix = matrix[:,self.active_sensors][self.active_motors]\n",
    "        return matrix\n",
    "    \n",
    "    def expanded_matrix(self,reduced_matrix):\n",
    "        matrix = np.zeros(self.shape)\n",
    "        flat = reduced_matrix.flatten()\n",
    "        matrix = np.zeros((14,28))\n",
    "        k = 0\n",
    "        for i in active_motors:\n",
    "            for j in active_sensors:\n",
    "                matrix[i,j] = flat[k]\n",
    "                k += 1\n",
    "        return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding\n",
    "\n",
    "# HTM SDR Scalar Encoder\n",
    "# Input: Scalar\n",
    "# Parameters: n - number of units, w - bits used to represent signal (width), b - buckets (i.e. resolution), \n",
    "#             min - minimum value of input (inclusive), max - maximum input value (inclusive)\n",
    "class scalar_sdr:\n",
    "    \n",
    "    def __init__(self, b, w, min_, max_, shape=(1,1), neg=True):\n",
    "        if type(b) != int or type(w) != int or type(min_) != float or type(max_) != float:\n",
    "            raise TypeError(\"b - buckets must be int, w - width must be int, min_ must be float and max_ must be float\")\n",
    "        self.b = b # must be int\n",
    "        self.w = w # must be int\n",
    "        self.min = min_ # must be float\n",
    "        self.max = max_ # must be float\n",
    "        self.n = b+w-1 # number of units for encoding\n",
    "        self.ndarray_shape = shape\n",
    "        self.nodes = self.n*reduce(lambda x, y: x*y, self.ndarray_shape)\n",
    "        self.neg = neg\n",
    "        \n",
    "    def encode(self,input_):\n",
    "        if input_ > self.max or input_ < self.min:\n",
    "            raise ValueError(\"Input outside encoder range!\")\n",
    "        if type(input_) != float:\n",
    "            raise TypeError(\"Input must be float!\")\n",
    "        if self.neg:\n",
    "            output = np.zeros(self.n)-1\n",
    "        else:\n",
    "            output = np.zeros(self.n)\n",
    "        index = int((input_-self.min)/(self.max-self.min)*self.b)\n",
    "        output[index:index+self.w] = 1\n",
    "        return output\n",
    "    \n",
    "    def encode_ndarray(self,input_):\n",
    "        if input_.shape != self.ndarray_shape:\n",
    "            raise ValueError(\"Input dimensions do not match specified encoder dimensions!\")\n",
    "        output = []\n",
    "        for i in np.nditer(input_, order='K'):\n",
    "            output.append(self.encode(float(i)))\n",
    "        return np.array(output)\n",
    "    '''\n",
    "    def decode(self,input_):\n",
    "        if len(input_) != self.n: # or len(np.nonzero(input_+1)[0]) != self.w: <-- Can't have since the network is not guaranteed to produce this by any means!!!\n",
    "            raise TypeError(\"Input does not correspond to encoder encoded data!\")\n",
    "        # output = np.nonzero(input_+1)[0][0]/float(self.b)*(self.max-self.min)+self.min <-- This doesn't work really since bits can randomly fire, taking the average is a more reasonable decoding\n",
    "        median = np.median(np.nonzero(input_+1)[0])            \n",
    "        try:\n",
    "            output = int(median-float(self.w)/2.0)/float(self.b)*(self.max-self.min)+self.min # i.e. figure out center (median more outlier resistant than mean) and subtract width/2\n",
    "        except ValueError:\n",
    "            output = None\n",
    "        return output\n",
    "    '''\n",
    "    def decode(self,input_):\n",
    "        if len(input_) != self.n: \n",
    "            raise TypeError(\"Input length does not match encoder length!\")\n",
    "        if len(np.nonzero(input_+1)[0]) == 0:\n",
    "            return np.nan\n",
    "        max_ = 0\n",
    "        output = 0.0\n",
    "        for i in range(self.b):\n",
    "            x = np.zeros(self.n)-1\n",
    "            x[i:i+self.w] = 1\n",
    "            if x.shape != input_.shape:\n",
    "                input_ = input_.reshape(x.shape)\n",
    "            score = np.inner(x,input_) # this was broken for some input formats \n",
    "            if score > max_:\n",
    "                max_ = score\n",
    "                output = float(i)/float(self.b)*(self.max-self.min)+self.min\n",
    "        return output\n",
    "            \n",
    "    def decode_ndarray(self,input_):\n",
    "        if input_.shape != (reduce(lambda x, y: x*y, self.ndarray_shape)*self.n,): \n",
    "            raise ValueError(\"Input dimensions do not match specified encoder dimensions!\")\n",
    "        input_ = input_.reshape(self.ndarray_shape+(self.n,))\n",
    "        output = []\n",
    "        for i in np.ndindex(self.ndarray_shape):\n",
    "            output.append(self.decode(input_[i]))\n",
    "        output = np.array(output).reshape(self.ndarray_shape)\n",
    "        return output\n",
    "    \n",
    "    def set_ndarray_shape(self,shape):\n",
    "        if type(shape) != tuple:\n",
    "            raise TypeError(\"Must provide tuple of array dimensions!\")\n",
    "        self.ndarray_shape = shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LAM\n",
    "\n",
    "class LAM:\n",
    "    def __init__(self,shape,weights=None):\n",
    "        self.shape = shape\n",
    "        try:\n",
    "            if weights == None:\n",
    "                self.weights = np.zeros(shape)\n",
    "        except ValueError:\n",
    "            self.weights = weights\n",
    "    \n",
    "    def store(self,input,output):\n",
    "        dW = np.outer(input,output)\n",
    "        self.weights += dW\n",
    "        \n",
    "    def recall(self,input,threshold=0,print_=False):\n",
    "        output = input.dot(self.weights)-threshold\n",
    "        if print_ == True:\n",
    "            print output\n",
    "        output[output > 0] = 1\n",
    "        output[output < 0] = 0\n",
    "        return output\n",
    "\n",
    "    def save_weights(self,filename):\n",
    "        np.save(filename, self.weights)\n",
    "        \n",
    "    def load_weights(self,filename):\n",
    "        weights = np.load(filename)\n",
    "        if weights.shape == self.shape:\n",
    "            self.weights = weights\n",
    "        else:\n",
    "            raise ValueError(\"Dimensions of specified weight array does not match network weight dimensions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first generate a current state of the behaviors stored in the brain, the state of the encoder being used\n",
    "# and the state of the memory networks\n",
    "\n",
    "# Brain encoder\n",
    "brain_encoder = scalar_sdr(1000,21,0.,1000.,neg=False)\n",
    "\n",
    "# Behaviors stored in brain and ids associated with them\n",
    "names = [\"zero\",\"fb\",\"fs\",\"sd\"]\n",
    "# Store the brain ids with no overlap, but with a fair amount of width.\n",
    "# This results in perfect memory recall and noise robustness.\n",
    "# The draw back is that essentially each id has its own isolated inputs and connections, \n",
    "# while sharing a common output with the other ids. The encoder in essence consequently serves as\n",
    "# a selection mechanism, which is quite questionable in terms of biological plausibility.\n",
    "brain_ids = [float(i)*brain_encoder.w for i in range(len(names))]\n",
    "behaviors = dict(zip(names,brain_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix Data\n",
    "\n",
    "active_motors = [1,3,4,5,10,12]\n",
    "expander = matrix_expansion(active_motors)\n",
    "\n",
    "# Front back\n",
    "filename = \"/home/markus/dep/dep_matrices/front_back.dep\"\n",
    "fb_matrix = expander.load_from_file(filename)\n",
    "fb_reduced = expander.reduced_matrix(fb_matrix)\n",
    "#fb_expanded = expander.expanded_matrix(fb_reduced)\n",
    "\n",
    "# Front side\n",
    "filename = \"/home/markus/dep/dep_matrices/front_side.dep\"\n",
    "fs_matrix = expander.load_from_file(filename)\n",
    "fs_reduced = expander.reduced_matrix(fs_matrix)\n",
    "#fs_expanded = expander.expanded_matrix(fs_reduced)\n",
    "\n",
    "# Side down\n",
    "filename = \"/home/markus/dep/dep_matrices/side_down.dep\"\n",
    "sd_matrix = expander.load_from_file(filename)\n",
    "sd_reduced = expander.reduced_matrix(sd_matrix)\n",
    "#sd_expanded = expander.expanded_matrix(sd_reduced)\n",
    "\n",
    "# Zero\n",
    "zero_matrix = np.zeros(fb_matrix.shape)\n",
    "zero_reduced = np.zeros(fb_reduced.shape)\n",
    "\n",
    "matrices = {\"fb\": fb_matrix, \"fs\": fs_matrix, \"sd\": sd_matrix, \"zero\": zero_matrix}\n",
    "#matrices = {\"fb\": fb_reduced, \"fs\": fs_reduced, \"sd\": sd_reduced, \"zero\": zero_reduced}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brain id to matrix memory network\n",
    "\n",
    "active_motors = [1,3,4,5,10,12]\n",
    "n = len(active_motors)\n",
    "matrix_shape = (n,n*2)\n",
    "matrix_encoder = scalar_sdr(100,21,-0.25,0.25,matrix_shape,neg=False)\n",
    "shape = (brain_encoder.n,matrix_encoder.n*reduce(lambda x, y: x*y, m_encoder.ndarray_shape))\n",
    "\n",
    "matrix_memories = LAM(shape)\n",
    "for id_ in behaviors:    \n",
    "    brain_sig = brain_encoder.encode(behaviors[id_])\n",
    "    matrix = m_encoder.encode_ndarray(matrices[id_])\n",
    "    matrix_memories.store(brain_sig,matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need function that does DEP calculation from input to output\n",
    "\n",
    "with tf.name_scope(\"input\"):\n",
    "    input_ = tf.placeholder(tf.float32, [None,28], name=\"input\") #differential sensors\n",
    "with tf.name_scope(\"weights\"):\n",
    "    weights = tf.placeholder(tf.float32, [None,14,28], name=\"weights\")\n",
    "with tf.name_scope(\"output\"):\n",
    "    in_ = tf.reduce_sum(weights*input_,axis=2)\n",
    "    out_ = tf.tanh(in_)\n",
    "    #out_ = tf.sigmoid(in_)\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "def dep_out(sensors, matrix):\n",
    "    out = sess.run(out_,{input_: sensors, weights: matrix})\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets use the sine sweep behavior representation to obtain the behavior pattern\n",
    "class behavior_from_matrix:\n",
    "\n",
    "    def __init__(self,matrix):\n",
    "        self.matrix = matrix\n",
    "        self.pattern = self.getPattern()\n",
    "        \n",
    "    def getPattern(self):\n",
    "        steps = 100\n",
    "        sines = [np.sin(np.arange(steps)*2*np.pi/steps+i*2*np.pi/28) for i in range(28)]\n",
    "        data = np.array([dep_out(sensors.reshape(1,28),self.matrix.reshape(1,14,28)).reshape(14,) for sensors in np.array(sines).T])\n",
    "        return data\n",
    "\n",
    "    def closest_phase(self,state):\n",
    "        RMS = [np.sqrt(np.sum((self.pattern[i]-state)**2))for i in range(len(self.pattern))]\n",
    "        indices_sorted = np.argsort(RMS)\n",
    "        return -indices_sorted[0]\n",
    "    \n",
    "    def delayed_sensors(self,index,delay=50):\n",
    "        a = np.roll(self.pattern,index,axis=0)\n",
    "        return a[-delay:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb = pickle.load(open(\"/home/markus/dep/dep_data/bases/fb.pickle\",\"rb\"))\n",
    "fs = pickle.load(open(\"/home/markus/dep/dep_data/bases/fs.pickle\",\"rb\"))\n",
    "sd = pickle.load(open(\"/home/markus/dep/dep_data/bases/sd.pickle\", \"rb\"))\n",
    "zero = [np.zeros(fb[0].shape),np.zeros(fb[1].shape), np.zeros(fb[2].shape)]\n",
    "\n",
    "bases = {\"fb\": fb, \"fs\": fs, \"sd\": sd, \"zero\": zero}\n",
    "ts = {'fb_to_fs': 126, 'fb_to_sd': 126, ('fs','fb'): 128, , 'sd': 121, 'zero': 0}\n",
    "transitions = dict(bases.keys(),[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = dict(zip(behaviors.keys(),[{\"weights_pos\":[], \"weights_vel\":[]} for i in behaviors]))\n",
    "\n",
    "# Generate weights\n",
    "pos_encoder = scalar_sdr(41,3,-100000.0,100000.0,shape=(6,1),neg=False)\n",
    "vel_encoder = scalar_sdr(3,1,-70.0,70.0,shape=(6,1),neg=False)\n",
    "\n",
    "for id_ in weights:\n",
    "    for i in active_motors:\n",
    "        pos = pos_encoder.encode(float(bases[id_][0][ts[id_]][i])) # this gets the transition point motor status and encodes it\n",
    "        m_pos = pos # hebbian learning of weights with only direct connections\n",
    "        weights[id_][\"weights_pos\"].append(m_pos)\n",
    "        \n",
    "        vel = vel_encoder.encode(float(bases[id_][1][ts[id_]][i]))\n",
    "        m_vel = np.outer(pos,vel) # hebbian learning of weights\n",
    "        weights[id_][\"weights_vel\"].append(m_vel)\n",
    "\n",
    "weights[id_][\"weights_pos\"] = np.array(behvs[id_][\"weights_pos\"]).reshape(1,6,pos_encoder.n,1)\n",
    "weights[id_][\"weights_vel\"] = np.array(behvs[id_][\"weights_vel\"]).reshape(1,6,pos_encoder.n,vel_encoder.n)\n",
    "weights[\"pos_encoder\"] = pos_encoder\n",
    "weights[\"vel_encoder\"] = vel_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add a new id in the brain\n",
    "names.append(\"mix\")\n",
    "brain_ids.append(float((brain_id[-1]+1))*brain_encoder.w)\n",
    "behaviors = dict(zip(names,brain_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to update the existing brain_id to dep_matrix memory network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
